{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge 6: Multi-Source Learning Content Ingestion & Structured Output Generation\n"
      ],
      "metadata": {
        "id": "Kg3QSbGx38wb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install huggingface_hub transformers\n",
        "!pip -q install pdfplumber pypdf2 pymupdf pillow pytesseract\n",
        "!pip -q install yt-dlp ffmpeg-python\n",
        "!pip -q install git+https://github.com/openai/whisper.git\n",
        "!pip -q install gradio networkx matplotlib pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0E-MPlF4IPO",
        "outputId": "11ad1070-2e93-445f-a424-2324be99a021"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, json, gc, textwrap, subprocess\n",
        "from pathlib import Path\n",
        "\n",
        "import pdfplumber\n",
        "try:\n",
        "    import fitz  # from PyMuPDF\n",
        "    HAS_PYMUPDF = True\n",
        "except Exception:\n",
        "    fitz = None\n",
        "    HAS_PYMUPDF = False\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import whisper\n",
        "import ffmpeg\n",
        "\n",
        "from huggingface_hub import InferenceClient\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "o5zIT2TM6C6P"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚¨áÔ∏è Paste your token between quotes (keep it private; don't commit to GitHub)\n",
        "os.environ[\"HF_TOKEN\"] = os.environ.get(\"HF_TOKEN\", \"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\")\n",
        "\n",
        "# Open-source models via HF API\n",
        "OPEN_API_MODELS = {\n",
        "    \"Qwen2.5-1.5B-Instruct\": \"Qwen/Qwen2.5-1.5B-Instruct\",  # default\n",
        "    \"Qwen2.5-7B-Instruct\":   \"Qwen/Qwen2.5-7B-Instruct\",     # slower, better quality\n",
        "    \"TinyLlama-1.1B-Chat\":   \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"  # very fast, lighter quality\n",
        "}\n",
        "\n",
        "DEFAULT_API_MODEL_HUMAN = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "CURRENT_API_MODEL = OPEN_API_MODELS[DEFAULT_API_MODEL_HUMAN]\n",
        "\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\", None)\n",
        "assert HF_TOKEN and HF_TOKEN.startswith(\"hf_\"), \"Please set HF_TOKEN with your Hugging Face token.\"\n",
        "\n",
        "# Initialize client\n",
        "HF_CLIENT = InferenceClient(model=CURRENT_API_MODEL, token=HF_TOKEN)\n",
        "\n",
        "# Tokenizer for chunking only (no weights downloaded)\n",
        "TOKENIZER_CACHE = {}\n",
        "def get_tokenizer(model_id):\n",
        "    if model_id not in TOKENIZER_CACHE:\n",
        "        TOKENIZER_CACHE[model_id] = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "    return TOKENIZER_CACHE[model_id]\n",
        "\n",
        "TOKENIZER = get_tokenizer(CURRENT_API_MODEL)\n",
        "print(\"‚úÖ HF API ready with:\", CURRENT_API_MODEL)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "h1_7Upru7E0u",
        "outputId": "ac5d9145-261d-4e7d-e6c7-34af4c438d0c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'TinyLlama/TinyLlama-1.1B-Chat-v1.0'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3373054951.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mDEFAULT_API_MODEL_HUMAN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mCURRENT_API_MODEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOPEN_API_MODELS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDEFAULT_API_MODEL_HUMAN\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mHF_TOKEN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"HF_TOKEN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(s: str) -> str:\n",
        "    if not s: return \"\"\n",
        "    s = s.replace(\"\\xa0\",\" \").replace(\"\\t\",\" \")\n",
        "    s = re.sub(r\"[ ]{2,}\", \" \", s)\n",
        "    s = re.sub(r\"\\n{3,}\", \"\\n\\n\", s)\n",
        "    return s.strip()\n",
        "\n",
        "def read_pdf(path: str) -> str:\n",
        "    text = []\n",
        "    try:\n",
        "        with pdfplumber.open(path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                text.append(page.extract_text() or \"\")\n",
        "    except Exception:\n",
        "        pass\n",
        "    txt = \"\\n\".join(text).strip()\n",
        "\n",
        "    # Fallback to PyMuPDF if very short (extraction failed)\n",
        "    if (not txt or len(txt) < 200) and HAS_PYMUPDF:\n",
        "        try:\n",
        "            doc = fitz.open(path)\n",
        "            txt = \"\\n\".join([page.get_text() for page in doc])\n",
        "        except Exception:\n",
        "            pass\n",
        "    return clean_text(txt or \"\")\n",
        "\n",
        "def download_youtube_audio(url: str, out_path=\"temp_audio.m4a\"):\n",
        "    cmd = [\"yt-dlp\", \"-f\", \"bestaudio/best\", \"-x\", \"--audio-format\", \"m4a\", \"-o\", out_path, url]\n",
        "    subprocess.run(cmd, check=True)\n",
        "    return out_path\n",
        "\n",
        "def transcribe_audio_whisper(audio_path: str, model_size=\"base\"):\n",
        "    model = whisper.load_model(model_size)\n",
        "    result = model.transcribe(audio_path, fp16=False)\n",
        "    return clean_text(result.get(\"text\",\"\"))\n",
        "\n",
        "def youtube_to_text(url: str, whisper_size=\"base\"):\n",
        "    audio = download_youtube_audio(url)\n",
        "    return transcribe_audio_whisper(audio, model_size=whisper_size)"
      ],
      "metadata": {
        "id": "rc3_gLGc7o5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_hf_api_model(human_choice: str):\n",
        "    \"\"\"Switch the API model (e.g., Qwen 1.5B / 7B / TinyLlama).\"\"\"\n",
        "    global CURRENT_API_MODEL, HF_CLIENT, TOKENIZER\n",
        "    CURRENT_API_MODEL = OPEN_API_MODELS[human_choice]\n",
        "    HF_CLIENT = InferenceClient(model=CURRENT_API_MODEL, token=HF_TOKEN)\n",
        "    TOKENIZER = get_tokenizer(CURRENT_API_MODEL)\n",
        "    return f\"‚úÖ Using: {human_choice} ({CURRENT_API_MODEL})\"\n",
        "\n",
        "def extract_chat_content(resp):\n",
        "    \"\"\"\n",
        "    HF client returns an object with .choices[0].message.content (newer),\n",
        "    or a dict-like structure in older versions. Handle both.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        return resp.choices[0].message[\"content\"]\n",
        "    except Exception:\n",
        "        try:\n",
        "            return resp.choices[0].message.content\n",
        "        except Exception:\n",
        "            # Fallback: string-ish\n",
        "            return str(resp)\n",
        "\n",
        "def call_llm_api(prompt: str, max_new_tokens=384, temperature=0.15, system_prompt=\"You are a precise assistant.\"):\n",
        "    \"\"\"\n",
        "    Preferred: use chat_completion (provider maps Qwen to 'conversational').\n",
        "    Falls back to text_generation if needed.\n",
        "    \"\"\"\n",
        "    # Try chat completion\n",
        "    try:\n",
        "        resp = HF_CLIENT.chat_completion(\n",
        "            model=CURRENT_API_MODEL,\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_prompt},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            max_tokens=max_new_tokens,\n",
        "            temperature=temperature,\n",
        "        )\n",
        "        return extract_chat_content(resp)\n",
        "    except Exception as e_chat:\n",
        "        # Fallback: try text_generation for models that support it\n",
        "        try:\n",
        "            resp = HF_CLIENT.text_generation(\n",
        "                prompt,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                temperature=temperature,\n",
        "                do_sample=True,\n",
        "                repetition_penalty=1.1,\n",
        "                return_full_text=False,\n",
        "            )\n",
        "            return resp\n",
        "        except Exception as e_tg:\n",
        "            raise RuntimeError(f\"HF API failed (chat & text): {e_chat} | {e_tg}\")"
      ],
      "metadata": {
        "id": "AyPz88UT716K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "\n",
        "def chunk_by_tokens(text: str, tokenizer, max_tokens=800, overlap=80):\n",
        "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = []\n",
        "    i = 0\n",
        "    while i < len(ids):\n",
        "        j = min(i + max_tokens, len(ids))\n",
        "        chunk_ids = ids[i:j]\n",
        "        chunk_text = tokenizer.decode(chunk_ids)\n",
        "        chunks.append(chunk_text)\n",
        "        i = j - overlap\n",
        "        if i < 0: i = 0\n",
        "    return chunks\n",
        "\n",
        "def extract_json_from_text(txt: str):\n",
        "    \"\"\"\n",
        "    1) Grab first {...} or [...] block\n",
        "    2) json.loads ‚Üí ast.literal_eval\n",
        "    \"\"\"\n",
        "    import json, re\n",
        "    m = re.search(r'(\\{.*\\}|\\[.*\\])', txt, flags=re.S)\n",
        "    candidate = m.group(1) if m else txt.strip()\n",
        "    candidate = re.sub(r\",\\s*([}\\]])\", r\"\\1\", candidate)\n",
        "    try:\n",
        "        return json.loads(candidate)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return ast.literal_eval(candidate)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def llm_json(prompt: str, fallback=None, max_new_tokens=512, temperature=0.15):\n",
        "    resp = call_llm_api(prompt, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "    js = extract_json_from_text(resp)\n",
        "    if js is None:\n",
        "        return fallback\n",
        "    return js"
      ],
      "metadata": {
        "id": "7_CJ83L9_dGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_chunks(text: str, tokenizer, max_tokens=800):\n",
        "    chunks = chunk_by_tokens(text, tokenizer, max_tokens=max_tokens, overlap=80)\n",
        "    bullets = []\n",
        "    for i, ch in enumerate(chunks, 1):\n",
        "        prompt = f\"\"\"Summarize the following content in 5 concise bullet points (no preamble).\n",
        "Keep it factual and avoid repetition.\n",
        "\n",
        "CONTENT (part {i}/{len(chunks)}):\n",
        "{ch}\n",
        "\n",
        "Bullets:\"\"\"\n",
        "        out = call_llm_api(prompt, max_new_tokens=256)\n",
        "        bullets.append(out.strip())\n",
        "    joined = \"\\n\".join(bullets)[:8000]\n",
        "    meta_prompt = f\"\"\"Unify these bullets into a single concise summary with 7 bullets:\n",
        "\n",
        "BULLETS:\n",
        "{joined}\n",
        "\n",
        "Final 7 bullets:\"\"\"\n",
        "    final = call_llm_api(meta_prompt, max_new_tokens=256)\n",
        "    return final.strip()\n",
        "\n",
        "def extract_topics_hierarchy(text: str, top_n=20, children_per_topic=5):\n",
        "    prompt = f\"\"\"Extract key learning topics and a hierarchy from the content below.\n",
        "Return STRICT JSON with this schema:\n",
        "{{\n",
        "  \"topics\": [\n",
        "    {{\n",
        "      \"name\": \"string\",\n",
        "      \"subtopics\": [\"string\", ...],\n",
        "      \"description\": \"one-sentence definition\"\n",
        "    }}, ...\n",
        "  ]\n",
        "}}\n",
        "Rules:\n",
        "- Include up to {top_n} topics\n",
        "- Each topic may have up to {children_per_topic} subtopics\n",
        "- Use concise phrases, no duplicates\n",
        "\n",
        "CONTENT:\n",
        "{text[:8000]}\n",
        "\n",
        "JSON ONLY:\"\"\"\n",
        "    fallback = {\"topics\": []}\n",
        "    js = llm_json(prompt, fallback=fallback, max_new_tokens=512)\n",
        "    # Validate & dedupe\n",
        "    if not isinstance(js, dict) or \"topics\" not in js:\n",
        "        js = fallback\n",
        "    seen = set()\n",
        "    clean_topics = []\n",
        "    for t in js.get(\"topics\", []):\n",
        "        name = (t.get(\"name\") or \"\").strip()\n",
        "        if name and name.lower() not in seen:\n",
        "            seen.add(name.lower())\n",
        "            st = t.get(\"subtopics\") or []\n",
        "            st = [s for s in st if isinstance(s, str) and s.strip()][:children_per_topic]\n",
        "            clean_topics.append({\n",
        "                \"name\": name,\n",
        "                \"subtopics\": st,\n",
        "                \"description\": (t.get(\"description\") or \"\").strip()\n",
        "            })\n",
        "    return {\"topics\": clean_topics[:top_n]}\n",
        "\n",
        "def generate_flashcards(text: str, n_cards=20):\n",
        "    prompt = f\"\"\"Create {n_cards} study flashcards from the content below.\n",
        "Return STRICT JSON array only (no other text).\n",
        "Each item: {{\"question\": \"Q?\", \"answer\": \"A.\", \"topic\": \"short tag\"}}.\n",
        "Keep answers 1‚Äì3 sentences, factual, no overlap.\n",
        "\n",
        "CONTENT:\n",
        "{text[:8000]}\n",
        "\n",
        "JSON ARRAY ONLY:\"\"\"\n",
        "    fallback = []\n",
        "    js = llm_json(prompt, fallback=fallback, max_new_tokens=768)\n",
        "    # Validate\n",
        "    cards = []\n",
        "    if isinstance(js, list):\n",
        "        for item in js[:n_cards]:\n",
        "            if isinstance(item, dict):\n",
        "                q = (item.get(\"question\") or \"\").strip()\n",
        "                a = (item.get(\"answer\") or \"\").strip()\n",
        "                tp = (item.get(\"topic\") or \"\").strip() or \"general\"\n",
        "                if q and a:\n",
        "                    cards.append({\"question\": q, \"answer\": a, \"topic\": tp})\n",
        "    if not cards:\n",
        "        cards = [{\"question\": \"What is the main idea?\",\n",
        "                  \"answer\": \"Summarize the central concept and its practical implications.\",\n",
        "                  \"topic\": \"overview\"}]\n",
        "    return cards[:n_cards]"
      ],
      "metadata": {
        "id": "6m3xK-tf_eqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_concept_graph(topics_json: dict, out_path=\"outputs/concept_graph.png\"):\n",
        "    G = nx.DiGraph()\n",
        "    for t in topics_json.get(\"topics\", []):\n",
        "        topic = t[\"name\"]\n",
        "        G.add_node(topic)\n",
        "        for sub in t.get(\"subtopics\", []):\n",
        "            if sub.strip():\n",
        "                G.add_node(sub)\n",
        "                G.add_edge(topic, sub)\n",
        "    plt.figure(figsize=(10,8))\n",
        "    pos = nx.spring_layout(G, seed=42, k=0.65)\n",
        "    nx.draw(\n",
        "        G, pos,\n",
        "        with_labels=True,\n",
        "        node_color='lightblue', node_size=1300,\n",
        "        font_size=9, arrows=True, arrowstyle='-|>', width=1.2\n",
        "    )\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path, dpi=200)\n",
        "    plt.close()\n",
        "    return out_path"
      ],
      "metadata": {
        "id": "vUBIpOKy_lGQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_corpus(text: str, top_n_topics=18, children_per_topic=4, n_cards=20):\n",
        "    text = clean_text(text)\n",
        "    summary = summarize_chunks(text, TOKENIZER)\n",
        "    topics = extract_topics_hierarchy(text, top_n=top_n_topics, children_per_topic=children_per_topic)\n",
        "    cards = generate_flashcards(text, n_cards=n_cards)\n",
        "\n",
        "    summary_json_path = \"outputs/summary.json\"\n",
        "    topics_json_path = \"outputs/topics.json\"\n",
        "    cards_csv_path = \"outputs/flashcards.csv\"\n",
        "    graph_png_path = \"outputs/concept_graph.png\"\n",
        "\n",
        "    with open(summary_json_path, \"w\") as f:\n",
        "        json.dump({\"summary\": summary}, f, indent=2)\n",
        "    with open(topics_json_path, \"w\") as f:\n",
        "        json.dump(topics, f, indent=2)\n",
        "    pd.DataFrame(cards).to_csv(cards_csv_path, index=False)\n",
        "    build_concept_graph(topics, graph_png_path)\n",
        "\n",
        "    return {\n",
        "        \"summary\": summary,\n",
        "        \"topics\": topics,\n",
        "        \"flashcards_csv\": cards_csv_path,\n",
        "        \"concept_graph_png\": graph_png_path,\n",
        "        \"summary_json\": summary_json_path,\n",
        "        \"topics_json\": topics_json_path\n",
        "    }"
      ],
      "metadata": {
        "id": "m07Qym7q_31K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gradio as gr\n",
        "\n",
        "# def select_engine(api_model_choice):\n",
        "#     msg = set_hf_api_model(api_model_choice)\n",
        "#     return msg\n",
        "\n",
        "# def process_input(pdf_file, youtube_url, raw_text, whisper_size, top_n, children, n_cards):\n",
        "#     corpus = \"\"\n",
        "#     try:\n",
        "#         if pdf_file is not None:\n",
        "#             corpus = read_pdf(pdf_file.name)\n",
        "#         elif youtube_url and youtube_url.strip():\n",
        "#             corpus = youtube_to_text(youtube_url.strip(), whisper_size=whisper_size)\n",
        "#         elif raw_text and raw_text.strip():\n",
        "#             corpus = clean_text(raw_text.strip())\n",
        "#         else:\n",
        "#             return \"Please provide PDF, YouTube URL, or paste text.\", \"\", \"\", None\n",
        "\n",
        "#         if not corpus or len(corpus) < 50:\n",
        "#             return \"The input appears empty or too short after extraction.\", \"\", \"\", None\n",
        "\n",
        "#         result = process_corpus(\n",
        "#             corpus,\n",
        "#             top_n_topics=int(top_n),\n",
        "#             children_per_topic=int(children),\n",
        "#             n_cards=int(n_cards)\n",
        "#         )\n",
        "#         return (\n",
        "#             result[\"summary\"],\n",
        "#             json.dumps(result[\"topics\"], indent=2),\n",
        "#             result[\"flashcards_csv\"],\n",
        "#             result[\"concept_graph_png\"]\n",
        "#         )\n",
        "#     except Exception as e:\n",
        "#         return f\"Error: {e}\", \"\", \"\", None\n",
        "\n",
        "# with gr.Blocks() as demo:\n",
        "#     gr.Markdown(\"## üìö Multi‚ÄëSource Learning Artifact Generator (Qwen via Hugging Face API)\")\n",
        "#     with gr.Row():\n",
        "#         api_model = gr.Dropdown(choices=list(OPEN_API_MODELS.keys()),\n",
        "#                                 value=DEFAULT_API_MODEL_HUMAN, label=\"HF API Model\")\n",
        "#         engine_btn = gr.Button(\"Use Selected Model\")\n",
        "#         engine_status = gr.Markdown(f\"Using: {DEFAULT_API_MODEL_HUMAN} ({CURRENT_API_MODEL})\")\n",
        "#     engine_btn.click(select_engine, inputs=[api_model], outputs=[engine_status])\n",
        "\n",
        "#     gr.Markdown(\"### Input Source\")\n",
        "#     with gr.Row():\n",
        "#         pdf_in = gr.File(label=\"Upload PDF (optional)\")\n",
        "#         yt_in = gr.Textbox(label=\"YouTube URL (optional)\")\n",
        "#     txt_in = gr.Textbox(label=\"Or paste transcript/text\", lines=8, placeholder=\"Paste raw text if not using PDF/YouTube\")\n",
        "\n",
        "#     with gr.Row():\n",
        "#         whisper_size = gr.Dropdown(choices=[\"tiny\",\"base\",\"small\"], value=\"base\", label=\"Whisper model (for YouTube audio)\")\n",
        "#         top_n = gr.Slider(5, 30, value=18, step=1, label=\"Max Topics\")\n",
        "#         children = gr.Slider(2, 8, value=4, step=1, label=\"Subtopics per Topic\")\n",
        "#         n_cards = gr.Slider(5, 40, value=20, step=1, label=\"Flashcards Count\")\n",
        "\n",
        "#     run_btn = gr.Button(\"Process\")\n",
        "#     gr.Markdown(\"---\")\n",
        "\n",
        "#     summary_out = gr.Textbox(label=\"Summary (bullets)\", lines=12)\n",
        "#     topics_out = gr.Textbox(label=\"Topics Hierarchy (JSON)\", lines=12)\n",
        "#     cards_download = gr.File(label=\"Flashcards CSV\")\n",
        "#     graph_img = gr.Image(label=\"Concept Graph (PNG)\")\n",
        "\n",
        "#     run_btn.click(\n",
        "#         fn=process_input,\n",
        "#         inputs=[pdf_in, yt_in, txt_in, whisper_size, top_n, children, n_cards],\n",
        "#         outputs=[summary_out, topics_out, cards_download, graph_img]\n",
        "#     )\n",
        "\n",
        "# demo.launch(debug=False)"
      ],
      "metadata": {
        "id": "hMalbcIN_49v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sample = \"\"\"Transformers use attention mechanisms to weigh relationships between tokens.\n",
        "They enable parallel processing and have become the backbone for language models.\n",
        "Key components include multi-head self-attention, positional encoding, and feed-forward layers.\"\"\"\n",
        "print(set_hf_api_model(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"))\n",
        "res = process_corpus(sample, top_n_topics=8, children_per_topic=3, n_cards=8)\n",
        "print(res[\"summary\"][:400])\n",
        "print(res[\"flashcards_csv\"], res[\"concept_graph_png\"])\n"
      ],
      "metadata": {
        "id": "jgoY2bGbMwy8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}